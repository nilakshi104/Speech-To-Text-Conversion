{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(path):\n",
    "    \"\"\"\n",
    "    Loads data from given path to .pkl file.\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "audio_sequence_padded=load_pkl('preprocessed_np1/audio_seq_padded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_sequence_length=np.load('preprocessed_np1/txt_length.npy')\n",
    "txt_sequence_padded=np.load('preprocessed_np1/txt_seq_padded.npy')\n",
    "audio_sequence_length=np.load('preprocessed_np1/audio_length.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2word=load_pkl('preprocessed_np1/ind2word')\n",
    "word2ind=load_pkl('preprocessed_np1/word2ind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_sequence_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "y=txt_sequence_padded\n",
    "X=audio_sequence_padded\n",
    "X_len=audio_sequence_length\n",
    "y_len=txt_sequence_length\n",
    "\n",
    "number_of_batches = (len(X)+batch_size-1)//batch_size\n",
    "sample_index = np.arange(len(X))\n",
    "val_batches=int(np.floor(number_of_batches*0.2))\n",
    "train_batches=number_of_batches-val_batches\n",
    "\n",
    "def batch_generator(batch):\n",
    "    x_batch,y_batch,x_batchel_len,y_batchel_len=[],[],[],[]\n",
    "    if batch == (number_of_batches-batch_size):\n",
    "        batch_index=sample_index[batch_size*batch::]\n",
    "        print('end')\n",
    "    else:\n",
    "        batch_index=sample_index[batch_size*batch:batch_size*(batch+1)]\n",
    "        \n",
    "    for i in (batch_index):\n",
    "        x_batch.append(X[i].toarray())\n",
    "        x_batchel_len.append(X_len[i])\n",
    "        y_batch.append(y[i])\n",
    "        y_batchel_len.append(y_len[i])\n",
    "    \n",
    "    y_batch=np.array(y_batch)\n",
    "#     y_batch=y_batch.astype('int32')\n",
    "    y_batch=torch.tensor(y_batch)\n",
    "    \n",
    "    x_batch=np.array(x_batch)\n",
    "#     x_batch=x_batch.astype('float64')\n",
    "    x_batch=torch.tensor(x_batch).float()\n",
    "    \n",
    "    return x_batch,y_batch,torch.tensor(x_batchel_len),torch.tensor(y_batchel_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape=(2,1628,494)\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,n_layers=1,dropout=0):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.n_layers=n_layers\n",
    "        self.hidden_size=hidden_size\n",
    "#         self.embedding=nn.Embedding(len(word2ind),hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "       \n",
    "    def forward(self,input_seq,input_lengths,hidden=None):\n",
    "#         F.normalize(input_seq,p=2,dim=2)\n",
    "        packed=nn.utils.rnn.pack_padded_sequence(input_seq,input_lengths,batch_first=True,enforce_sorted=False)  #enforce_sorted=False) since arranged in asscending order\n",
    "#         packed=nn.utils.rnn.pack_padded_sequence(input_seq,input_lengths,batch_first=True)\n",
    "        outputs,hidden=self.gru(packed,hidden)\n",
    "        outputs, _ =nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs=outputs[:,:,:self.hidden_size]+outputs[:,:,self.hidden_size:]\n",
    "        return outputs,hidden\n",
    "\n",
    "# encoder=EncoderRNN(494)\n",
    "# op,hidden=encoder(x_in,xl)\n",
    "# print(op.shape)\n",
    "# print(hidden.shape)\n",
    "# torch.sum(hidden*op,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self,method,hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "        self.method=method\n",
    "        if self.method not in ['dot','general','concat']:\n",
    "            raise ValueError(self.method,\"is not an appropriate attention method.\")\n",
    "        self.hidden_size=hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn=nn.Linear(self.hidden_size,self.hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn=nn.Linear(self.hidden_size * 2,hidden_size)\n",
    "            self.v=nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            \n",
    "    def dot_score(self,hidden,encoder_output):\n",
    "        return torch.sum(hidden*encoder_output,dim=2)\n",
    "        \n",
    "    def general_score(self,hidden,encoder_output):\n",
    "        energy=self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy,dim=2)            \n",
    "    \n",
    "    def concat_score(self,hidden,encoder_output):\n",
    "        energy=self.attn(torch.cat((hidden.expand(encoder_output.size(0),-1,-1),encoder_output),2)).tanh()\n",
    "        return torch.sum(self.v*energy,dim=2)\n",
    "    \n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "    # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden,encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden,encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden,encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()                            #attn_energies has size (hidden_size,max_len)\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies,dim=1).unsqueeze(1)             #(hidden_size,1,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,attn_model,hidden_size,output_size,n_layers=2,dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN,self).__init__()\n",
    "\n",
    "        #keep for reference \n",
    "        self.attn_model=attn_model\n",
    "        self.hidden_size=hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.n_layers=n_layers\n",
    "        self.dropout=dropout\n",
    "\n",
    "        #Define layers\n",
    "        self.embedding = nn.Embedding(len(word2ind), hidden_size)\n",
    "        self.embedding_dropout=nn.Dropout(dropout)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers==1 else dropout))\n",
    "        self.concat=nn.Linear(hidden_size * 2,hidden_size)\n",
    "        self.out=nn.Linear(hidden_size,output_size)\n",
    "        self.attn=Attn(attn_model,hidden_size)\n",
    "\n",
    "    def forward(self,input_step,last_hidden,encoder_outputs):\n",
    "        #we run one step(word) at a time\n",
    "        #Get embedding of current input word\n",
    "        embedded=self.embedding(input_step)\n",
    "        embedded=self.embedding_dropout(embedded)\n",
    "        #Forward through unidirectional GRU\n",
    "        rnn_output,hidden=self.gru(embedded,last_hidden)\n",
    "        #Calculate attention weights from current GRU output\n",
    "        attn_weights=self.attn(rnn_output,encoder_outputs)\n",
    "        #Multiply attention weights to encoder to get new \"weighted sum\" context vector\n",
    "        context=attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "        #Concatenate weighted context vector and GRU output using p(yt|y<t ,x) = softmax(Wsh̃t)\n",
    "        rnn_output=rnn_output.squeeze(0)\n",
    "        context=context.squeeze(1)\n",
    "        concat_input=torch.cat((rnn_output,context),1)\n",
    "        concat_output=torch.tanh(self.concat(concat_input))\n",
    "        #Predict next word using p(yt|y<t ,x) = softmax(Wsh̃t)\n",
    "        output=self.out(concat_output)\n",
    "        output=F.softmax(output,dim=1)\n",
    "        return output,hidden\n",
    "    \n",
    "# attn=Attn('dot',494)\n",
    "# decoder_input=torch.LongTensor([[1 for _ in range(batch_size)]])\n",
    "# Luong_attn=LuongAttnDecoderRNN('dot',494,len(word2ind))\n",
    "# decoder_hidden=hidden[:Luong_attn.n_layers] \n",
    "# o,h=Luong_attn(decoder_input,decoder_hidden,op)\n",
    "# print(o.shape)\n",
    "# print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_batch,x_len,y_batch,y_len,encoder,decoder,word2ind,encoder_optimizer,decoder_optimizer,batch_size):\n",
    "    encoder_optimizer.zero_grad()  #zero_grad clears old gradients from the last step (otherwise you'd just accumulate the gradients from all loss. backward() calls). ... step() causes the optimizer to take a step based on the gradients of the parameters.\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    #Set device options\n",
    "    x_batch=x_batch.to(device)\n",
    "    x_len=x_len.to(device)\n",
    "    y_batch=y_batch.to(device)\n",
    "    y_len=y_len.to(device)\n",
    "    \n",
    "    #Initialize variables\n",
    "    loss=0\n",
    "    num=0\n",
    "    print_losses=[]\n",
    "    \n",
    "    #Forward pass through encoder\n",
    "    encoder_outputs,encoder_hidden=encoder(x_batch,x_len)\n",
    "    \n",
    "    #Create initial decoder input (starting with SOS_token for each sent)\n",
    "    decoder_input=torch.LongTensor([[word2ind['<SOS>'] for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device) \n",
    "    \n",
    "    #Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden=encoder_hidden[:decoder.n_layers]           #':deocder.n_layer' used to access 0th index i.e. if n_layers =1 it will access [0,:,:] and if n_layers=2 it will access [:2,0,0]\n",
    "\n",
    "    #Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing=True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    max_target_len=max([indexes for indexes in y_len]) \n",
    "    \n",
    "    if use_teacher_forcing :\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output,decoder_hidden=decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            #Teacher forcing: next input is current target\n",
    "#             decoder_input = y_batch[t].view(1, -1)\n",
    "            decoder_input=(torch.tensor([y_batch[i][t] for i in range(batch_size)]).view(-1,batch_size)).to(device)\n",
    "#             decoder_input=(torch.tensor([y_batch[0][t],y_batch[1][t]]).view(-1,batch_size)).to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            for i in range(batch_size):\n",
    "                if y_batch[i][t]!=torch.tensor(0):\n",
    "                    num+=1\n",
    "                    decoder_output1=decoder_output[i,:].view(-1,decoder_output.shape[1])\n",
    "                    mask_loss = nn.CrossEntropyLoss()(decoder_output1.to(device), torch.tensor([y_batch[i][t]]).to(device))\n",
    "                    loss += mask_loss\n",
    "                    \n",
    "        loss=loss/num\n",
    "    \n",
    "            \n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            for i in range(batch_size):\n",
    "                if y_batch[i][t]!=torch.tensor(0):\n",
    "                    num+=1\n",
    "                    decoder_output1=decoder_output[i,:].view(-1,decoder_output.shape[1])\n",
    "                    mask_loss = nn.CrossEntropyLoss()(decoder_output1.to(device), torch.tensor([y_batch[i][t]]).to(device))\n",
    "                    loss += mask_loss\n",
    "        loss=loss/num\n",
    "            \n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_list=[]\n",
    "def trainIters(model_name,encoder,decoder,encoder_optimizer, decoder_optimizer, encoder_n_layers, decoder_n_layers,save_dir, clip, loadFilename,epoch,batch_size):\n",
    "    # Load batches for each iteration\n",
    "    print_loss=0\n",
    "    plot_loss=0\n",
    "    print_every=10\n",
    "    plot_every=9\n",
    "    start=time.time()\n",
    "    \n",
    "    if loadFilename:\n",
    "        print('started with : {}'.format(checkpoint['iteration']))\n",
    "        start_iteration = checkpoint['iteration'] + 1        #IF TRAINING IS DISCONNECTED INBETWEEN , WE NEED NOT TO TRAIN FROM SCRATCH AND CAN CONTINUE FROM WHERE IT IS LEFT BEFORE\n",
    "    \n",
    "    else:\n",
    "        start_iteration=0\n",
    "        print('STARTED')\n",
    "        \n",
    "    for _ in tqdm(range(epoch)): \n",
    "        for train_batch in range(start_iteration,val_batches+train_batches-1):\n",
    "            x_train,y_train,x_len,y_len=batch_generator(train_batch)\n",
    "            train_loss=train(x_train,x_len,y_train,y_len,encoder,decoder,word2ind,encoder_optimizer,decoder_optimizer,batch_size)\n",
    "            print_loss+=train_loss\n",
    "            plot_loss+=train_loss\n",
    "\n",
    "            if train_batch % print_every == 0 and train_batch !=0:\n",
    "                print_loss_avg = print_loss / (print_every+1)\n",
    "                print(\"Iteration: {}; Average loss: {:.4f};time:{}\".format(train_batch, print_loss_avg,time.time()-start))\n",
    "                print('_'*30)\n",
    "                print_loss = 0\n",
    "\n",
    "            if train_batch % plot_every == 0 and train_batch !=0:\n",
    "                plot_loss_list.append(plot_loss/(plot_every+1))\n",
    "                plot_loss = 0\n",
    "                \n",
    "#         scheduler.step()\n",
    "#         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "#         decoder_scheduler = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer, val_batches+train_batches-1,eta_min=learning_rate*decoder_learning_ratio, last_epoch=5)\n",
    "\n",
    "\n",
    "    plt.plot(plot_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "model_name='speech2text_model'\n",
    "attn_model='dot'\n",
    "hidden_size=494\n",
    "encoder_n_layers=2\n",
    "decoder_n_layers=2\n",
    "dropout=0.1\n",
    "n_iteration_val=False\n",
    "\n",
    "# load_trained=input('enter True if using pretrained otherwise False')\n",
    "load_trained='False'\n",
    "if load_trained == 'True':\n",
    "    print('Using Trained Model')\n",
    "#     checkpoint_iter = 15000\n",
    "#     loadFilename = os.path.join('/processed_np', model_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "    #If loading on same machine the model was trained on\n",
    "#     checkpoint = torch.load(loadFilename,device)\n",
    "#     # If loading a model trained on GPU to CPU\n",
    "#     #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "#     encoder_optimizer_sd = checkpoint['en_opt']\n",
    "#     decoder_optimizer_sd = checkpoint['de_opt']\n",
    "#     embedding_sd = checkpoint['embedding']\n",
    "#     n_iteration_val=True \n",
    "    \n",
    "else:\n",
    "    loadFilename=None\n",
    "    \n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, len(word2ind), decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.4\n",
    "learning_rate = 0.00001\n",
    "decoder_learning_ratio = 5.0\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "# encoder_scheduler = optim.lr_scheduler.CosineAnnealingLR(encoder_optimizer, 1 ,eta_min=learning_rate, last_epoch=5)\n",
    "\n",
    "# decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate*decoder_learning_ratio)\n",
    "# decoder_scheduler = optim.lr_scheduler.CosineAnnealingLR(decoder_optimizer, val_batches+train_batches-1,eta_min=learning_rate*decoder_learning_ratio, last_epoch=5)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "\n",
    "epoch=4\n",
    "\n",
    "#95 IS Y_BATCH[1]\n",
    "trainIters(model_name,encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "               encoder_n_layers, decoder_n_layers, 'processed_np/',\n",
    "               clip, loadFilename,epoch,batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binaryMatrix(l,value=word2ind['<PAD>']):\n",
    "#     m=[]\n",
    "#     print(l)\n",
    "#     for i,seq in enumerate(l):\n",
    "#         m.append([])\n",
    "#         for token in seq:\n",
    "#             if token == value:\n",
    "#                 m[i].append(0)\n",
    "#             else:\n",
    "#                 m[i].append(1)\n",
    "#     return m \n",
    "\n",
    "# def maskNLLLoss(op,target):\n",
    "#     mask=binaryMatrix(target)                 #mask is in form 1,0\n",
    "#     mask=torch.BoolTensor(mask) \n",
    "#     nTotal=mask.sum()\n",
    "#     crossEntropy=-torch.log(torch.gather(op,1,target.view(-1,1)).squeeze(1))                 #loss=summation(log(probabilities))\n",
    "#     loss=crossEntropy.masked_select(mask).mean()\n",
    "#     loss=loss.to(device)\n",
    "#     return loss,nTotal.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
