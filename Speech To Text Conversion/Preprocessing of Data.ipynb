{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python_speech_features\n",
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO UNTAR FILE\n",
    "# !tar -xvf openslr_dataset1.tar.xz\n",
    "# librosa.load('openslr_dataset1/LibriSpeech/dev-clean/1673/143396/1673-143396-0000.flac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from python_speech_features import mfcc\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioToInputVector(audio_filename,numcep,numcontext):\n",
    "    \"\"\"Given a WAV audio file at ``audio_filename``, calculates ``numcep`` MFCC features\n",
    "    at every 0.01s time step with a window length of 0.025s. Appends ``numcontext``\n",
    "    context frames to the left and right of each time step, and returns this data\n",
    "    in a numpy array.\"\"\"\n",
    "    audio,fs=librosa.load(audio_filename)\n",
    "    #get mfcc coeficients\n",
    "    features=mfcc(audio,samplerate=fs,numcep=numcep,nfft=551)  #NUMCEP=26,NFFT=512\n",
    "    #we only keep every second feature (BiRNN stride=2)\n",
    "    features=features[::2]            \n",
    "    #one stride per time step in input\n",
    "    num_strides=len(features)\n",
    "    #add empty initial and final context\n",
    "    empty_context=np.zeros((numcontext,numcep),dtype=features.dtype)\n",
    "    features=np.concatenate((empty_context,features,empty_context))\n",
    "    #create a view into the array with overlapping strides of size numcontext (past) + 1(present)+numcontext(future)\n",
    "    window_size=2*numcontext+1\n",
    "    train_inputs=np.lib.stride_tricks.as_strided(\n",
    "                 features,\n",
    "                 (num_strides,window_size,numcep),\n",
    "                 (features.strides[0],features.strides[0],features.strides[1]),\n",
    "                 writeable=False)\n",
    "    \n",
    "    #Flatten the second and third dimensions\n",
    "    train_inputs=np.reshape(train_inputs,[num_strides,-1])\n",
    "    \n",
    "    #Copy strided array so that we can write to it safely\n",
    "    train_inputs = np.copy(train_inputs)\n",
    "    train_inputs=(train_inputs-np.mean(train_inputs))/np.std(train_inputs)\n",
    "    \n",
    "    #RETURN :)\n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dir_path: path to the directory with txt and audio files.\n",
    "        how_many: Integer. Number of directories we want to iterate,\n",
    "                  that contain the audio files and transcriptions.\n",
    "    Returns:\n",
    "        txts: The spoken texts extracted from the .txt files,\n",
    "              which correspond to the .flac files in audios.\n",
    "              Text version.\n",
    "        audios: The .flac file paths corresponding to the\n",
    "                sentences in txts. Spoken version.\n",
    "    \"\"\"\n",
    "    dir_path = Path(dir_path)\n",
    "    txt_list = [f for f in dir_path.glob('**/*.txt') if f.is_file()]\n",
    "    audio_list = [f for f in dir_path.glob('**/*.flac') if f.is_file()]\n",
    "\n",
    "    print('Number of audio txt paths:', len(txt_list))\n",
    "    print('Number of audio file paths:', len(audio_list))\n",
    "\n",
    "    txts = []\n",
    "    audios = []\n",
    "    audio_paths = []\n",
    "\n",
    "    for i, txt in tqdm(enumerate(txt_list)):\n",
    "        with open(txt) as f:\n",
    "            for line in f.readlines():\n",
    "                for audio in audio_list:\n",
    "                    if audio.stem in line:\n",
    "                        line = re.sub(r'[^A-Za-z]', ' ', line)\n",
    "                        line = line.strip()\n",
    "                        txts.append(line)\n",
    "                        audios.append(audioToInputVector(audio, 26, 9))\n",
    "                        audio_paths.append(audio)\n",
    "                        break\n",
    "    return txts, audios, audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio txt paths: 97\n",
      "Number of audio file paths: 2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [08:23,  5.19s/it]\n"
     ]
    }
   ],
   "source": [
    "txts,audios,audio_paths=load_data('openslr_dataset1/LibriSpeech/dev-clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_txts(txts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        txts: The texts that will be split\n",
    "              into single characters\n",
    "    Returns:\n",
    "        The splitted texts and array of all unique characters\n",
    "        in those texts.\n",
    "    \"\"\"\n",
    "    txts_splitted = []\n",
    "    word_list=[]\n",
    "    unique_words = set()      #if 3 sets are consider each of them can be updated by elements in other if used set1.update(set2) for other combinations\n",
    "\n",
    "    for txt in txts:\n",
    "        splitted=txt.split(' ')\n",
    "        txts_splitted.append(splitted)\n",
    "        unique_words.update(splitted)\n",
    "    return txts_splitted,unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b=split_txts(txts)\n",
    "# b=sorted(b)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_dicts(unique_words, specials=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        unique_chars: Set of unique chars appearning in texts.\n",
    "        specials: Special characters we want to add to the dict,\n",
    "                  such as <PAD>, <SOS> or <EOS>\n",
    "    Returns:\n",
    "        char2ind: look updict from character to index\n",
    "        ind2char: lookup dict from index to character\n",
    "    \"\"\"\n",
    "    word2ind = {}\n",
    "    ind2word = {}\n",
    "    i = 0\n",
    "\n",
    "    if specials is not None:\n",
    "        for sp in specials:\n",
    "            word2ind[sp] = i\n",
    "            ind2word[i] = sp\n",
    "            i += 1\n",
    "    for word in unique_words:\n",
    "        word2ind[word] = i\n",
    "        ind2word[i] = word\n",
    "        i += 1\n",
    "    return word2ind, ind2word\n",
    "\n",
    "def convert_txt_to_inds(txt, word2ind, eos=False, sos=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        txt: Array of chars to convert to inds.\n",
    "        char2ind: Lookup dict from chars to inds.\n",
    "    Returns: The converted chars, i.e. array of ints.\n",
    "    \"\"\"\n",
    "    txt_to_inds = [word2ind[word] for word in txt]\n",
    "    # for word in txt:\n",
    "    #   print(word)\n",
    "    if eos:\n",
    "        txt_to_inds.append(word2ind['<EOS>'])\n",
    "    if sos:\n",
    "        txt_to_inds.insert(0, word2ind['<SOS>'])\n",
    "    return txt_to_inds\n",
    "\n",
    "def convert_inds_to_txt(inds, ind2word):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inds: Array of ints to convert to chars\n",
    "        ind2char: Lookup dict from ind to chars\n",
    "    Returns: The converted inds, i.e. array of chars.\n",
    "    \"\"\"\n",
    "    inds_to_txt = [ind2word[ind] for ind in inds]\n",
    "    return inds_to_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txts(txts, specials):\n",
    "    \"\"\"\n",
    "    Processes the texts. Calls the functions split_txts,\n",
    "    create_lookup_dicts and uses convert_txt_to_inds.\n",
    "    Args:\n",
    "        txts: Array of strings. Input texts.\n",
    "        specials: Specials tokens we want to include in the\n",
    "                  lookup dicts\n",
    "    Returns:\n",
    "        txts_splitted: Array of the input texts splitted up into\n",
    "                       characters\n",
    "        unique_chars: Set of Unique chars appearing in input texts.\n",
    "        char2ind: Lookup dict from character to index.\n",
    "        ind2char: Lookup dict from index to character.\n",
    "        txts_converted: txts splitted converted to indices of\n",
    "                        word2ind. i.e. array of arrays of ints.\n",
    "    \"\"\"\n",
    "    txts_splitted, unique_words = split_txts(txts)\n",
    "  \n",
    "    word2ind, ind2word = create_lookup_dicts(unique_words, specials)\n",
    "\n",
    "    txts_converted = [convert_txt_to_inds(txt, word2ind, eos=True, sos=True)\n",
    "                      for txt in txts_splitted]\n",
    "\n",
    "    return txts_splitted, unique_words, word2ind, ind2word, txts_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts_splitted, unique_words, word2ind, ind2word, txts_converted=process_txts(txts,['<PAD>','<SOS>','<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_length(audios,\n",
    "                   txts,\n",
    "                   audio_paths,\n",
    "                   txts_splitted,\n",
    "                   txts_converted,\n",
    "                   by_text_length=True):\n",
    "    \"\"\"\n",
    "    Sort texts by text length from shortest to longest.\n",
    "    To keep everything in order we also sort the rest of the data.\n",
    "    Args:\n",
    "        by_text_length: Boolean. Sort either by text lengths or\n",
    "                        by length of audios.\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    # check if that works. if not audios isn't a  numpy array.\n",
    "    # in that case we could convert beforehand.\n",
    "    if by_text_length:\n",
    "#         indices = [txt[0] for txt in sorted(enumerate(txts_converted),reverse=True, key=lambda x: len(x[1]))]\n",
    "        indices = [txt[0] for txt in sorted(enumerate(txts_converted),key=lambda x: len(x[1]))]\n",
    "    else:\n",
    "#         indices = [a[0] for a in sorted(enumerate(audios), reverse=True,key=lambda x: x[1].shape[0])]\n",
    "        indices = [a[0] for a in sorted(enumerate(audios), key=lambda x: x[1].shape[0])]\n",
    "\n",
    "#         indices = [txt[0] for txt in sorted(enumerate(txts_converted),reverse=True, key=lambda x: len(x[1]))]\n",
    "\n",
    "    txts_sorted = np.array(txts)[indices]\n",
    "    audios_sorted = np.array(audios)[indices]\n",
    "    audio_paths_sorted = np.array(audio_paths)[indices]\n",
    "    txts_splitted_sorted = np.array(txts_splitted)[indices]\n",
    "    txts_converted_sorted = np.array(txts_converted)[indices]\n",
    "\n",
    "    return txts_sorted, audios_sorted, audio_paths_sorted, txts_splitted_sorted, txts_converted_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts_sorted,audios_sorted,audio_paths_sorted,txts_splitted_sorted,txts_converted_sorted=sort_by_length(audios,txts,audio_paths,txts_splitted,txts_converted,by_text_length=False)  #since audio is input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_txt_sequences(sequences,pad_tok):\n",
    "    \"\"\"pads the sentences,so that all sentences in a batch have same length\"\"\"\n",
    "    max_length=max(len(x) for x in sequences)\n",
    "    sequence_padded,sequence_length=[],[]\n",
    "    for seq in tqdm(sequences):\n",
    "        seq1 = seq + [pad_tok] * max(max_length - len(seq), 0)\n",
    "\n",
    "        sequence_padded.append(seq1)\n",
    "        sequence_length.append(len(seq))\n",
    "\n",
    "    return np.array(sequence_padded), sequence_length   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio_sequences(sequences, tail=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: Array of audio sequences\n",
    "        tail: Boolean. Append silence to end or beginning\n",
    "    Returns: Padded array with audio sequences, padded with\n",
    "             silence.\n",
    "    \"\"\"\n",
    "\n",
    "    max_length = max(seq.shape[0] for seq in sequences)\n",
    "    sequences_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in tqdm(sequences):\n",
    "        if tail:\n",
    "            seq_shape = seq.shape\n",
    "            pad_vector = [0] * seq_shape[1]\n",
    "            n_vectors_to_add = max_length - seq_shape[0]\n",
    "\n",
    "            for _ in range(n_vectors_to_add):\n",
    "                seq = np.append(seq, [pad_vector], axis=0)\n",
    "\n",
    "                \n",
    "        #FOR NUMPY TO SPARSE ONLY FOR XTRAIN,XVAL NOT YTRAIN AND YVAL\n",
    "        sequences_padded.append(csr_matrix(seq))\n",
    "        sequence_length.append(seq_shape[0])\n",
    "\n",
    "\n",
    "    return sequences_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [00:00<00:00, 351824.84it/s]\n"
     ]
    }
   ],
   "source": [
    "txt_sequence_padded, txt_sequence_length=pad_txt_sequences(txts_converted_sorted,word2ind['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [34:35<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_sequence_padded, audio_sequence_length=pad_audio_sequences(audios_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_as_pickled_object(obj, filepath):\n",
    "    \"\"\"\n",
    "    This is a defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "    https://stackoverflow.com/questions/31468117/python-3-can-pickle-handle-byte-objects-larger-than-4gb\n",
    "    \"\"\"\n",
    "    max_bytes = 2 ** 31 - 1\n",
    "    bytes_out = pickle.dumps(obj)\n",
    "    n_bytes = sys.getsizeof(bytes_out)\n",
    "    with open(filepath, 'wb') as f_out:\n",
    "        for idx in range(0, n_bytes, max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx + max_bytes])\n",
    "            \n",
    "def read_as_pickled_object(filepath):\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(file_path)\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "    data2 = pickle.loads(bytes_in)\n",
    "    \n",
    "            \n",
    "def write_pkl(path, data):\n",
    "    \"\"\"\n",
    "    Writes the given data to .pkl file.\n",
    "    \"\"\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_pkl(path):\n",
    "    \"\"\"\n",
    "    Loads data from given path to .pkl file.\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickled_object(audio_sequence_padded,'preprocessed_np2/audio_seq_padded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('preprocessed_np2/txt_seq_padded',txt_sequence_padded)\n",
    "np.save('preprocessed_np2/txt_length',txt_sequence_length)\n",
    "np.save('preprocessed_np2/audio_length',audio_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pkl('preprocessed_np2/word2ind', word2ind)\n",
    "write_pkl('preprocessed_np2/ind2word', ind2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
